24年末——至今的笔记整理</br>
网课老师：李宏毅团队，吴恩达，李沐，“知乎博主”等</br>
AI理论实践:</br>
数据处理管道:</br>
&emsp;&emsp;自监督学习的方式，设计了自适应阈值动态伪标签机制，增强了数据信号。</br>
&emsp;&emsp;语音序列数据处理。对不同长度的语音特征，进行填充分段采样，使同批次内所有序列具有相同长度。</br>
&emsp;&emsp;机器翻译项目中，针对中英文文本数据集，进行语料库清洗与筛选和文本转换为子词序列，然后使用unigram算法在双语数据上联合训练，使之后的模型能够处理罕见词和未登录词。最后将分词后的文本转换为二进制格式，配置共享词典和多进程处理。</br>

模型架构设计：</br>
&emsp;&emsp;实现多层全连接网络+多种激活函数，引入ImprovedSequential优化网络。通过模块化设计支持不同网络架构的快速实验和对比。</br>
&emsp;&emsp;机器翻译项目中，实现了注意力增强的RNN序列到序列模型：通过双向编码和单向解码架构结合注意力机制，解决长序列依赖问题；开发增量推理和状态缓存优化，提升生成效率；为机器翻译和文本生成任务提供高效的端到端解决方案。</br>

系统项目：</br>
&emsp;&emsp;AIAgent:构建了基于树形搜索的自主代码生成系统：设计三阶段迭代框架(draft→debug→improve)，通过智能策略平衡探索与利用；实现多进程安全沙箱执行环境，集成完整的异常捕获与超时控制；开发LLM驱动的代码生成与自修复管道，实现从任务理解到结果评估的全流程自动化；建立经验记忆系统持续优化代码质量，在机器学习任务中实现完全自主的解决方案开发。</br></br>
&emsp;&emsp;brief_RAG：构建了多智能体协同的搜索增强问答系统：设计三阶段专业化管道（问题精炼→关键词提取→知识增强回答），通过角色分工提升任务处理精度；实现异步网络检索引擎，集成HEAD预检查、编码检测和内容清洗，确保高质量实时知识获取；开发统一的LLM服务架构，支持GPU加速推理和健壮异常处理；建立端到端问答流水线，在复杂问题场景中实现准确高效的信息整合与回答生成。</br></br>
&emsp;&emsp;Fine-tuning:构建了特定样本的大模型微调系统：在云端Linux系统上高效微调Llama模型；设计自监督数据增强机制，基于特定风格自动生成高质量训练样本；开发字词敏感的提示工程，通过Few-shot示例和角色扮演引导模型掌握五言绝句创作规范；实现两阶段课程学习流水线，显著提升模型在指定风格任务完成度。</br></br>
&emsp;&emsp;Fine-tuning:构建了数学推理的大模型微调系统：在云端Linux系统上高效微调Llama模型；设计思维链增强的提示工程和动态n-shot示例选择；建立完整的训练优化流水线，集成梯度检查点、TF32加速和多进程数据加载；开发多任务评估框架，在GSM8K和AILuminate数据集上实现竞争力的数学问题求解精度。</br></br>
&emsp;&emsp;machin_transformer:开发了机器翻译架构：设计包含数据加载、模型训练、验证评估、推理生成的完整流水线；实现训练稳定性优化策略（标签平滑、梯度裁剪、学习率调度）；构建评估体系（SacreBLEU集成、多维度监控）；建立工程化实验管理（配置系统、检查点管理、可复现性保障），在英中翻译任务中实现稳定的模型训练。</br></br>
&emsp;&emsp;RLHF:构建了基于直接偏好优化(DPO)的RLHF对齐系统：实现端到端的偏好学习流水线，通过支持/反对样本对引导模型掌握标准；设计可控对齐实验机制，支持不同数据比例和训练强度的效果对比；建立完整的基线评估-对齐训练-效果验证流程，显著提升了模型的安全性。</br>





  

